{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\n# n.Data were preprocess from this kernel prhttps://www.kaggle.com/holydark30/cleaned-data\n# If decide to use this one,duplicate code below doesn't need to run.\n#cleaned_train = pd.read_csv('/kaggle/input/tj-data/cleaned_train.csv')\n#cleaned_test = pd.read_csv('/kaggle/input/tj-data/cleaned_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tj19data/train.csv')\ntest = pd.read_csv('/kaggle/input/tj19data/test.csv')\ndem = pd.read_csv('/kaggle/input/tj19data/demo.csv')\ntxn = pd.read_csv('/kaggle/input/tj19data/txn.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txn[['n3','n4']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txn.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txn[txn['n4'] < 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txn.dtypes\n# Convert type to make data process faster.\ntxn.astype({'id': 'int32', 'old_cc_label':'int8', \n            'c5': 'uint8','c6':'int16','c7':'uint8',\n            'n3':'uint16','n5':'uint16', 'n6':'uint16','n7':'uint16'}).dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txn['month'] = pd.to_datetime(txn['n3'], format='%j').dt.strftime('%m')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"limit = dem['n2'].dropna().unique() \ndem['n1'].fillna(0, inplace=True)\ndef find_nearest_above(target, my_array=limit):\n    diff = my_array - target\n    mask = np.ma.less_equal(diff, 0)\n    # We need to mask the negative differences and zero\n    # since we are looking for values above\n    if np.all(mask):\n        return None # returns None if target is greater than any value\n    masked_diff = np.ma.masked_array(diff, mask)\n    return my_array[masked_diff.argmin()]\ndem['n2'] = dem.apply(lambda x: find_nearest_above(target=x['n1']) if pd.isnull(x['n2']) else x['n2'], axis=1)\ndem['rat_n1_n2'] = dem['n1']/dem['n2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleaning(df):\n    temp = df.copy()\n    temp = pd.merge(temp, txn.groupby(['id'])['old_cc_label'].agg(lambda x: pd.Series.mode(x)[0]), on=\"id\",how=\"left\")\n    temp = pd.merge(temp, txn.groupby(['id'])['n7'].agg(lambda x: pd.Series.mode(x)[0]), on=\"id\",how=\"left\")\n    temp = pd.merge(temp, txn.groupby(['id'])['c5'].agg(lambda x: pd.Series.mode(x)[0]), on=\"id\",how=\"left\")\n    temp = pd.merge(temp, txn.groupby(['id'])['c6'].agg(lambda x: pd.Series.mode(x)[0]), on=\"id\",how=\"left\")\n    temp = pd.merge(temp, txn.groupby(['id'])['c7'].agg(lambda x: pd.Series.mode(x)[0]), on=\"id\",how=\"left\")\n    \n    group_month = txn.groupby(['id','month'])[['n4']].agg(['mean' , 'sum'])\n    group_month.columns = ['n4_month_mean','n4_month_sum']\n    group_month.reset_index(inplace=True)\n    \n    uniq_month = group_month['month'].unique()\n    for month in uniq_month:\n        t = group_month[group_month['month'] == month].copy()\n        t.drop('month',axis=1, inplace=True)\n        t.columns = ['id','n4_month_mean_'+month,'n4_month_sum_'+month]\n        temp = pd.merge(temp,t, on=\"id\",how=\"left\")\n        \n    group_month = txn.groupby(['id'])[['n4']].agg(['mean' , 'sum']).reset_index()\n    group_month.columns = ['id','n4_count_mean','n4_month_sum']\n    temp = pd.merge(temp, group_month,on=\"id\",how=\"left\")\n    \n    # Transaction frequency \n    count = txn.groupby('id').size().reset_index()\n    count.columns = ['id','freq']\n    temp = pd.merge(temp, count, on=\"id\",how=\"left\")\n    \n    # Work on feature engineer n5, and n6.\n\n    t = txn.groupby(['id'])[['n5','n6']].agg(['sum','mean']).reset_index()\n    t.columns = ['id', 'n5_sum','n5_mean','n6_sum','n6_mean']\n    \n    temp = pd.merge(temp, t, on=\"id\", how=\"left\")\n    \n    # Transaction used on certain service/purchase.\n    #txt = txn['t0'].value_counts()\n    #len(txn)\n    # Select top 20 common consumption,\n    #txt_feat = list(txt[:20].index)\n\n    \n    temp = pd.merge(temp, dem, on=\"id\", how=\"left\")\n    temp.fillna(0, inplace=True)\n    print(len(temp))\n    return temp\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_train = cleaning(train)\ncleaned_test = cleaning(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(dem['n1'].fillna(0)/dem['n2'].dropna()).describe() \n# The ratio between n1 and n2 has a mean of lesser than 1, however max as large as 725...\n# if n1 NaN is 0, and n2 can be dropped as low\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dem[dem['n1'] > dem['n2']]\n# There are some case n1 > n2.. Possibly outlier?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels_w = np.array(train['label'].value_counts().sort_index()/len(train))\nlabels_w","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = np.sort(train['label'].unique())\nlabels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_train = cleaned_train.append([cleaned_train[cleaned_train['label'] == 5]]*4,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils.class_weight import compute_sample_weight\nfrom sklearn.metrics import log_loss\ntrain = pd.read_csv('/kaggle/input/tj19data/train.csv')\nw = train['label'].value_counts().sort_index() / len(train)\nclass_weights = list()\nxgb_cw = dict()\nfor i in range(13):\n    xgb_cw[i] = w[i]\n    w_d = {0:1, 1:w[i]}\n    class_weights.append(w_d)\n    \n\ndef wmcl(F_prob, A):\n    samp_w = compute_sample_weight(xgb_cw, A)\n    return np.mean(log_loss(A, F_prob,sample_weight=samp_w))\n\n\ndef xgb_wmcl(y_pred, y_train):\n    label = y_train.get_label()\n    return 'logloss', wmcl(y_pred, label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom imblearn.over_sampling import SMOTE\nimport xgboost as xgb\n\nX_train, X_test, y_train, y_test = train_test_split(cleaned_train.drop(['label', 'id'], axis=1), \n                                                    cleaned_train['label'], \n                                                    test_size=0.2, random_state=42, \n                                                    stratify=cleaned_train['label'])\n\n#xgb_model = xgb.XGBClassifier(random_state=42)\n\n#xgb_model.fit(X_train, y_train, sample_weight=compute_sample_weight(xgb_cw, y_train))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#preds_prob = xgb_model.predict_proba(X_train)\n#print(\"Train metric \" + str(wmcl(preds_prob,y_train.values)))\n\n\n#preds_prob = xgb_model.predict_proba(X_test)\n#print(\"Test metric \" + str(wmcl(preds_prob, y_test.values)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_feat = ['c0','c1','c2','c3','c4','c5','c6','c7']\nxgb_train = xgb.DMatrix(X_train.values, label=y_train.values, weight=compute_sample_weight(xgb_cw, y_train.values))\nxgb_val = xgb.DMatrix(X_test.values)\nxgb_eval = xgb.DMatrix(X_test.values, label=y_test.values, weight=compute_sample_weight(xgb_cw, y_test.values))\n\n\n\"\"\"\nN_FOLDS = 5\n\ndel hyperparameters['n_estimators']\nhyperparameters['seed']= 42\nhyperparameters['silent']= True\nhyperparameters['nthread']= -1\nhyperparameters['num_class'] = 13\n\n# Perform cross validation with early stopping\ncv_results = xgb.cv(hyperparameters, \n                    xgb_train, num_boost_round = 100, \n                    nfold = N_FOLDS, stratified=True,feval=xgb_wmcl,\n                    early_stopping_rounds = 100,\n                    verbose_eval = False, seed = 42, maximize=False)\nmax_idx = np.argmin(cv_results['test-logloss-mean'])\n# Extract the best score\n\n# Boosting rounds that returned the highest cv score\nn_estimators = max_idx+1\n\n# Add the number of estimators to the hyperparameters\nhyperparameters['n_estimators'] = n_estimators\n\n# Highest score\nbest = cv_results['test-logloss-mean'][max_idx]\n\n# Standard deviation of best score\nbest_std = cv_results['test-logloss-std'][max_idx]\n\n\nprint('The most optimized logloss in cross validation was {:.5f} with std of {:.5f}.'.format(best, best_std))\nprint('The ideal number of iterations was {}.'.format(n_estimators)) \"\"\"\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"csw = compute_sample_weight(xgb_cw, y_train.values)\ne_csw = compute_sample_weight(xgb_cw, y_test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import hp\nimport csv\nfrom hyperopt import STATUS_OK\nfrom timeit import default_timer as timer\n\nN_FOLDS = 5\n\ndef objective(hyperparameters):\n    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Optimization.\n       Writes a new line to `outfile` on every iteration\"\"\"\n    \n    hyperparameters['silent'] = True\n    \n    # Keep track of evals\n    global ITERATION\n    \n    ITERATION += 1\n    \n    # Using early stopping to find number of trees trained\n\n    hyperparameters['n_estimators']=300\n    \n    \n    # Make sure parameters that need to be integers are integers\n    for parameter_name in ['max_depth', 'min_child_weight','random_state']:\n        hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n\n    start = timer()\n        \n    hyperparameters['seed']= 42\n    hyperparameters['silent']= True\n    hyperparameters['nthread']= -1\n    \n    \n    # Perform cross validation with early stopping\n    model = xgb.XGBClassifier(**hyperparameters)\n    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], \n              early_stopping_rounds=25, sample_weight=csw,\n              eval_metric=xgb_wmcl,\n              sample_weight_eval_set=[e_csw])\n\n    # Highest score\n    loss = wmcl(model.predict_proba(X_test), y_test)\n\n\n    # Boosting rounds that returned the highest cv score\n    n_estimators = model.best_iteration\n\n    # Add the number of estimators to the hyperparameters\n    hyperparameters['n_estimators'] = n_estimators\n\n\n    run_time = timer() - start\n\n    # Loss must be minimized\n    best_score = 100-loss\n    \n\n    # Write to the csv file ('a' means append)\n    of_connection = open(OUT_FILE, 'a')\n    writer = csv.writer(of_connection)\n    writer.writerow([loss, hyperparameters, ITERATION, run_time, best_score])\n    of_connection.close()\n\n    # Dictionary with information for evaluation\n    return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n            'train_time': run_time, 'status': STATUS_OK}\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the search space\nspace = {\n    'num_class':13,\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.5)),\n    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.3, 1.0),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'gamma': hp.uniform('gamma', 0.0, 0.5),\n    'max_depth': hp.quniform('max_depth', 5, 15, 1),\n    'random_state': 42,\n    'objective':'multi:softprob'\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from hyperopt import tpe\nfrom hyperopt import Trials\n\n# Create the algorithm\ntpe_algorithm = tpe.suggest\n# Record results\ntrials = Trials()\n\n# Create a file and open a connection\nOUT_FILE = 'bayes_test.csv'\nof_connection = open(OUT_FILE, 'w')\nwriter = csv.writer(of_connection)\n\nITERATION = 2\n\n# Write column names\nheaders = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\nwriter.writerow(headers)\nof_connection.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\"\"\"\nfrom hyperopt import fmin\n# Global variable\nglobal  ITERATION\n\nITERATION = 2\nMAX_EVALS = 30\n# Run optimization\nbest = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,\n            max_evals = MAX_EVALS)\n\nbest\n\ntrials_dict = sorted(trials.results, key = lambda x: x['loss'])\nresults = pd.read_csv(OUT_FILE)\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import ast\nimport shap\ndef evaluate(results, name):\n    \"\"\"Evaluate model on test data using hyperparameters in results\n       Return dataframe of hyperparameters\"\"\"\n    \n    new_results = results.copy()\n    # String to dictionary\n    new_results['hyperparameters'] = new_results['hyperparameters'].map(ast.literal_eval)\n    \n    # Sort with best values on top\n    new_results = new_results.sort_values('score', ascending = False).reset_index(drop = True)\n    \n    # Print out cross validation high score\n    print('The highest cross validation score from {} was {:.5f} found on iteration {}.'.format(name, new_results.loc[0, 'score'], new_results.loc[0, 'iteration']))\n    \n    # Use best hyperparameters to create a model\n    hyperparameters = new_results.loc[0, 'hyperparameters']\n    model = xgb.XGBClassifier(**hyperparameters)\n    model.fit(X_train, y_train, \n              early_stopping_rounds=25, \n              sample_weight=csw)\n    \n    # Train and make predictions\n    preds = model.predict(X_test)\n    \n    print('log-loss from {} on test data = {:.5f}.'.format(name, wmcl(model.predict_proba(X_test), y_test)))\n    \n    # Create dataframe of hyperparameters\n    hyp_df = pd.DataFrame(columns = list(new_results.loc[0, 'hyperparameters'].keys()))\n\n    # Iterate through each set of hyperparameters that were evaluated\n    for i, hyp in enumerate(new_results['hyperparameters']):\n        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]), \n                               ignore_index = True)\n        \n    # Put the iteration and score in the hyperparameter dataframe\n    hyp_df['iteration'] = new_results['iteration']\n    hyp_df['score'] = new_results['score']\n    \n    return hyp_df\n\n#bayes_results = evaluate(results, name = 'Bayesian')\n#bayes_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_results = results.copy()\n# String to dictionary\nnew_results['hyperparameters'] = new_results['hyperparameters'].map(ast.literal_eval)\n\n# Sort with best values on top\nnew_results = new_results.sort_values('score', ascending = False).reset_index(drop = True)\n\n# Use best hyperparameters to create a model\nhyperparameters = new_results.loc[0, 'hyperparameters']\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hyperparameters = {'colsample_bytree': 0.3193923647239065,\n 'gamma': 0.33134122637800645,\n 'learning_rate': 0.0824265818290931,\n 'max_depth': 13,\n 'min_child_weight': 3,\n 'num_class': 13,\n 'objective': 'multi:softprob',\n 'random_state': 42,\n 'reg_alpha': 0.4137163898530162,\n 'reg_lambda': 0.6494331857571901,\n 'silent': True,\n 'n_estimators': 507,\n 'seed': 42,\n 'nthread': -1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = xgb.XGBClassifier(**hyperparameters)\nmodel.fit(X_train, y_train, eval_set=[(X_test, y_test)], \n          early_stopping_rounds=25, sample_weight=csw,\n          eval_metric=xgb_wmcl,\n          sample_weight_eval_set=[e_csw])\n\n# Highest score\nloss = wmcl(model.predict_proba(X_test), y_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\n# Use Tree SHAP algorithms to explain the output of ensemble tree models\nexplainer = shap.TreeExplainer(model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values = explainer.shap_values(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, feature_names=X_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The first argument is the index of the feature we want to plot\n# The second argument is the matrix of SHAP values (it is the same shape as the data matrix)\n# The third argument is the data matrix (a pandas dataframe or numpy array)\nshap.dependence_plot(0, shap_values, X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict_proba(cleaned_test.drop('id',axis=1))\ncol = ['class'+str(i) for i in range(13)]\nsample_sub = pd.DataFrame(preds, columns=col)\nsample_sub.insert(0,'id',cleaned_test['id'])\nsample_sub.to_csv('O_1033.csv', index = False, header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['label'], order=[10,12,0,11,8,4,7,9,2,6,3,5],orient='')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}